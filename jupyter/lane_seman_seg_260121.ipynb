{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"139KaWLxzYm1Dv68DzlXs2r4uHNlAClmk","timestamp":1769144359577}],"machine_shape":"hm","gpuType":"L4","mount_file_id":"1_bVGSqan9GRGExuoYbSaI6TEQkfb7JZo","authorship_tag":"ABX9TyMynttblAxTyJw51s1rp4P0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yovF7Q-xzGbh","executionInfo":{"status":"ok","timestamp":1769145099285,"user_tz":-540,"elapsed":224175,"user":{"displayName":"함성민","userId":"11551094569326456083"}},"outputId":"fa36ea95-891b-4213-96fa-437dedc508f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Copying zip to local...\n","zip local: /content/ds_cache/SDLane.zip (8334.9 MB), copy_time=124.0s\n","Unzipping...\n","Extracted to: /content/ds_cache/SDLane\n","Top-level entries (sample):\n"," - /content/ds_cache/SDLane/SDLane\n"," - /content/ds_cache/SDLane/.unzipped_done\n"]}],"source":["\n","from pathlib import Path\n","import shutil, zipfile, time\n","\n","# 승호님 드라이브\n","#ZIP_DRIVE1 = Path(\"/content/drive/MyDrive/DataSet/MultiCameraSemanticSegmentation_Left,Label.zip\")\n","#ZIP_DRIVE2 = Path(\"/content/drive/MyDrive/DataSet/MonoCameraSemanticSegmentation.zip\")\n","\n","# 자체 드라이브\n","ZIP_DRIVE1 = Path(\"/content/drive/MyDrive/datasets/SDLane.zip\")\n","#ZIP_DRIVE2 = Path(\"/content/drive/MyDrive/datasets/MonoCameraSemanticSegmentation.zip\")\n","#ZIP_DRIVE3 = Path(\"/content/drive/MyDrive/datasets/MultiCameraSemanticSegmentation_g_labels.zip\")\n","CACHE_DIR = Path(\"/content/ds_cache\")\n","ZIP_LOCAL1 = CACHE_DIR / \"SDLane.zip\" # SDLane 기준 복사 107초\n","EXTRACT_DIR1 = CACHE_DIR / \"SDLane\" # 압축풀기 130초\n","#ZIP_LOCAL2 = CACHE_DIR / \"MCSSeg_mono.zip\"\n","#EXTRACT_DIR2 = CACHE_DIR / \"MCSSeg_mono\"\n","#ZIP_LOCAL3 = CACHE_DIR / \"g_labels.zip\"\n","#EXTRACT_DIR3 = CACHE_DIR\n","\n","CACHE_DIR.mkdir(parents=True, exist_ok=True)\n","EXTRACT_DIR1.mkdir(parents=True, exist_ok=True)\n","#EXTRACT_DIR2.mkdir(parents=True, exist_ok=True)\n","\n","\n","assert ZIP_DRIVE1.exists(), f\"ZIP not found: {ZIP_DRIVE1}\"\n","# 1) zip을 로컬로 복사\n","t0 = time.time()\n","if not ZIP_LOCAL1.exists() or ZIP_LOCAL1.stat().st_size != ZIP_DRIVE1.stat().st_size:\n","    print(\"Copying zip to local...\")\n","    shutil.copy2(ZIP_DRIVE1, ZIP_LOCAL1)\n","#    shutil.copy2(ZIP_DRIVE2, ZIP_LOCAL2)\n","    #shutil.copy2(ZIP_DRIVE3, ZIP_LOCAL3)\n","\n","print(f\"zip local: {ZIP_LOCAL1} ({ZIP_LOCAL1.stat().st_size/1024/1024:.1f} MB), copy_time={time.time()-t0:.1f}s\")\n","\n","# 2) 압축 해제 (이미 풀려있으면 스킵)\n","marker = EXTRACT_DIR1 / \".unzipped_done\"\n","if not marker.exists():\n","    print(\"Unzipping...\")\n","    with zipfile.ZipFile(ZIP_LOCAL1, \"r\") as zf:\n","        zf.extractall(EXTRACT_DIR1)\n","    #with zipfile.ZipFile(ZIP_LOCAL2, \"r\") as zf:\n","    #    zf.extractall(EXTRACT_DIR2)\n","    #with zipfile.ZipFile(ZIP_LOCAL3, \"r\") as zf:\n","    #    zf.extractall(EXTRACT_DIR3)\n","    marker.write_text(\"ok\")\n","print(\"Extracted to:\", EXTRACT_DIR1)\n","0\n","# 3) 내부 구조 빠르게 확인 (상위 몇 개만)\n","top = list(EXTRACT_DIR1.glob(\"*\"))[:20]\n","print(\"Top-level entries (sample):\")\n","for p in top:\n","    print(\" -\", p)"]},{"cell_type":"code","source":[],"metadata":{"id":"5-zZQx-MP52f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# DeepLabV3+ (semantic) for SDLane (polygon/polyline JSON) - Single Colab Cell (TRAIN ONLY)\n","# - Train: /content/ds_cache/SDLane/SDLane/train/{images,labels}/<hash>/*.jpg|.png|.json\n","# - Val  : /content/ds_cache/SDLane/SDLane/test/{images,labels}/<hash>/*.jpg|.png|.json\n","# - Exclude bad train folder: a03dce1fc941e29b7a692717e40ca88d7c3aa18e\n","# - GT: union(binary) lane mask from geometries\n","# - Metrics: Dice/IoU + Boundary-F1 @ tolerance (2/4/8 px)\n","# - Saves: /content/drive/MyDrive/seman_seg_runs/<run>/ (epoch ckpt, best.pt, history)\n","# - Includes: profiling(fetch/H2D/compute/opt/imgs/s), torch.compile, channels_last, optional fixed crop\n","# ============================================================\n","\n","!pip -q install -U segmentation-models-pytorch timm\n","\n","import os, glob, json, math, time, random, shutil, subprocess\n","import numpy as np\n","from datetime import datetime\n","from PIL import Image\n","import cv2\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import segmentation_models_pytorch as smp\n","from tqdm import tqdm\n","\n","# ---------------------------\n","# 0) Run dir\n","# ---------------------------\n","\n","RUNS_ROOT = \"/content/drive/MyDrive/seman_seg_runs\"\n","run_name  = datetime.now().strftime(\"deeplabv3p_%Y%m%d_%H%M%S\")\n","RUN_DIR   = os.path.join(RUNS_ROOT, run_name)\n","CKPT_DIR  = os.path.join(RUN_DIR, \"checkpoints\")\n","os.makedirs(CKPT_DIR, exist_ok=True)\n","print(\"[RunDir]\", RUN_DIR)\n","\n","# ---------------------------\n","# 1) Config (Resume+Profile 코드 우선 세팅 반영)\n","# ---------------------------\n","SEED = 42\n","\n","TRAIN_ROOT = \"/content/ds_cache/SDLane/SDLane/train\"\n","VAL_ROOT   = \"/content/ds_cache/SDLane/SDLane/test\"\n","BAD_FOLDER = \"a03dce1fc941e29b7a692717e40ca88d7c3aa18e\"\n","\n","# Image sizing (keep aspect, moderate downscale)\n","RESIZE_SHORTEST = 1024\n","RESIZE_LONGEST  = 1920\n","SIZE_DIVISOR    = 32\n","\n","# Rasterization\n","LINE_WIDTH = 6\n","USE_AA = False\n","\n","# Training (아래 코드 우선: BS=16, LR=6e-4)\n","EPOCHS       = 13\n","TRAIN_BS     = 16\n","LR           = 6e-4\n","WEIGHT_DECAY = 1e-4\n","LAMBDA_DICE  = 0.5\n","POS_WEIGHT   = 8.0\n","\n","# Limits\n","MAX_TRAIN_SAMPLES = None\n","MAX_VAL_SAMPLES   = 500\n","\n","# Metrics\n","BND_TOLS = (2, 4, 8)\n","\n","# Profiling (아래 코드 우선)\n","PROFILE_EVERY = 100\n","SMI_EVERY     = 200\n","GRAD_ACCUM_STEPS = 1\n","\n","# Speed knobs (아래 코드 우선)\n","USE_COMPILE       = True\n","USE_CHANNELS_LAST = True\n","\n","# Optional fixed crop for train only\n","USE_TRAIN_FIXED_CROP = False\n","FIXED_CROP_SIZE = 1024\n","\n","# Model\n","ENCODER_NAME = \"resnet50\"\n","ENCODER_WTS  = \"imagenet\"\n","\n","# DataLoader workers\n","NUM_WORKERS = min(8, os.cpu_count() or 2)\n","\n","# Save config\n","cfg = dict(\n","    seed=SEED,\n","    train_root=TRAIN_ROOT, val_root=VAL_ROOT, bad_folder=BAD_FOLDER,\n","    resize_shortest=RESIZE_SHORTEST, resize_longest=RESIZE_LONGEST, size_divisor=SIZE_DIVISOR,\n","    line_width=LINE_WIDTH, use_aa=USE_AA,\n","    epochs=EPOCHS, train_bs=TRAIN_BS, lr=LR, weight_decay=WEIGHT_DECAY,\n","    lambda_dice=LAMBDA_DICE, pos_weight=POS_WEIGHT,\n","    bnd_tols=list(BND_TOLS),\n","    max_train_samples=MAX_TRAIN_SAMPLES, max_val_samples=MAX_VAL_SAMPLES,\n","    num_workers=NUM_WORKERS,\n","    profile_every=PROFILE_EVERY, smi_every=SMI_EVERY, grad_accum_steps=GRAD_ACCUM_STEPS,\n","    use_compile=USE_COMPILE, use_channels_last=USE_CHANNELS_LAST,\n","    use_train_fixed_crop=USE_TRAIN_FIXED_CROP, fixed_crop_size=FIXED_CROP_SIZE,\n","    encoder_name=ENCODER_NAME, encoder_weights=ENCODER_WTS,\n",")\n","with open(os.path.join(RUN_DIR, \"config.json\"), \"w\") as f:\n","    json.dump(cfg, f, indent=2)\n","\n","# ---------------------------\n","# 2) Reproducibility & speed knobs\n","# ---------------------------\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEED)\n","\n","cv2.setNumThreads(0)\n","torch.set_num_threads(1)\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.set_float32_matmul_precision(\"high\")\n","torch.backends.cudnn.benchmark = True\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"[Device]\", device, \"| workers:\", NUM_WORKERS, \"| BS:\", TRAIN_BS, \"| Accum:\", GRAD_ACCUM_STEPS)\n","\n","# AMP dtype: bf16(ampere+) else fp16 + GradScaler\n","AMP_DTYPE = None\n","USE_SCALER = False\n","if device.type == \"cuda\":\n","    major, _ = torch.cuda.get_device_capability()\n","    AMP_DTYPE = torch.bfloat16 if major >= 8 else torch.float16\n","    USE_SCALER = (AMP_DTYPE == torch.float16)\n","scaler = torch.cuda.amp.GradScaler(enabled=USE_SCALER)\n","print(f\"[AMP] dtype={AMP_DTYPE} | GradScaler={USE_SCALER}\")\n","\n","# ---------------------------\n","# 3) Utils: resize / pad\n","# ---------------------------\n","def resize_keep_aspect(H, W, shortest_edge, longest_edge):\n","    scale = shortest_edge / min(H, W)\n","    newH, newW = int(round(H * scale)), int(round(W * scale))\n","    if max(newH, newW) > longest_edge:\n","        scale = longest_edge / max(newH, newW)\n","        newH, newW = int(round(newH * scale)), int(round(newW * scale))\n","    return max(1,newH), max(1,newW)\n","\n","def pad_to_divisor_tensor(x, divisor=32, pad_value=0.0):\n","    # x: (C,H,W)\n","    C, H, W = x.shape\n","    padH = (divisor - H % divisor) % divisor\n","    padW = (divisor - W % divisor) % divisor\n","    if padH == 0 and padW == 0:\n","        return x, (0,0,0,0)\n","    x = F.pad(x, (0, padW, 0, padH), value=pad_value)\n","    return x, (0, padW, 0, padH)\n","\n","def pad_to_divisor_mask(y, divisor=32, pad_value=0):\n","    # y: (H,W)\n","    H, W = y.shape\n","    padH = (divisor - H % divisor) % divisor\n","    padW = (divisor - W % divisor) % divisor\n","    if padH == 0 and padW == 0:\n","        return y.long(), (0,0,0,0)\n","    yy = F.pad(y.unsqueeze(0).unsqueeze(0).float(), (0, padW, 0, padH), value=float(pad_value)).squeeze(0).squeeze(0)\n","    return yy.long(), (0, padW, 0, padH)\n","\n","# ---------------------------\n","# 4) Rasterize: JSON geometries -> union binary mask\n","# ---------------------------\n","def _is_closed_polygon(pts_xy, close_tol=5.0, min_area=50.0):\n","    if pts_xy.shape[0] < 3:\n","        return False\n","    p0, pN = pts_xy[0], pts_xy[-1]\n","    if np.linalg.norm(p0 - pN) > close_tol:\n","        return False\n","    area = cv2.contourArea(pts_xy.astype(np.float32))\n","    return abs(area) >= min_area\n","\n","def make_union_mask(json_path, H, W, line_width=6, use_aa=False):\n","    with open(json_path, \"r\") as f:\n","        ann = json.load(f)\n","    geoms = ann.get(\"geometry\", [])\n","    mask = np.zeros((H, W), dtype=np.uint8)\n","    lt = cv2.LINE_AA if use_aa else cv2.LINE_8\n","\n","    for g in geoms:\n","        if not g or len(g) < 2:\n","            continue\n","        pts = np.array(g, dtype=np.float32)\n","        pts_i = np.round(pts).astype(np.int32)\n","\n","        if _is_closed_polygon(pts, close_tol=5.0, min_area=50.0):\n","            cv2.fillPoly(mask, [pts_i], 1)\n","        else:\n","            cv2.polylines(mask, [pts_i], isClosed=False, color=1, thickness=line_width, lineType=lt)\n","    return mask\n","\n","# ---------------------------\n","# 5) Index dataset\n","# ---------------------------\n","def build_items(split_root, exclude_folder=None):\n","    img_root = os.path.join(split_root, \"images\")\n","    lbl_root = os.path.join(split_root, \"labels\")\n","    folders = sorted([d for d in os.listdir(img_root) if os.path.isdir(os.path.join(img_root, d))])\n","    items = []\n","    for folder in folders:\n","        if exclude_folder is not None and folder == exclude_folder:\n","            continue\n","        img_dir = os.path.join(img_root, folder)\n","        lbl_dir = os.path.join(lbl_root, folder)\n","        if not os.path.isdir(lbl_dir):\n","            continue\n","        img_paths = sorted(glob.glob(os.path.join(img_dir, \"*.jpg\"))) + sorted(glob.glob(os.path.join(img_dir, \"*.png\")))\n","        for img_path in img_paths:\n","            stem = os.path.splitext(os.path.basename(img_path))[0]\n","            json_path = os.path.join(lbl_dir, f\"{stem}.json\")\n","            if os.path.isfile(json_path):\n","                items.append((img_path, json_path))\n","    return items\n","\n","train_items = build_items(TRAIN_ROOT, exclude_folder=BAD_FOLDER)\n","val_items   = build_items(VAL_ROOT, exclude_folder=None)\n","\n","if MAX_TRAIN_SAMPLES is not None:\n","    train_items = train_items[:MAX_TRAIN_SAMPLES]\n","if MAX_VAL_SAMPLES is not None:\n","    val_items = val_items[:MAX_VAL_SAMPLES]\n","\n","print(f\"[Index] train={len(train_items)} | val={len(val_items)} | excluded={BAD_FOLDER}\")\n","\n","# ---------------------------\n","# 6) Dataset\n","# ---------------------------\n","IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n","IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n","\n","class SDLaneSemanticDataset(Dataset):\n","    def __init__(self, items, shortest_edge, longest_edge, size_divisor=32,\n","                 line_width=6, use_aa=False, return_orig_for_eval=False,\n","                 use_fixed_crop=False, fixed_crop_size=1024):\n","        self.items = items\n","        self.shortest_edge = shortest_edge\n","        self.longest_edge = longest_edge\n","        self.size_divisor = size_divisor\n","        self.line_width = line_width\n","        self.use_aa = use_aa\n","        self.return_orig_for_eval = return_orig_for_eval\n","        self.use_fixed_crop = use_fixed_crop\n","        self.fixed_crop_size = fixed_crop_size\n","\n","    def __len__(self):\n","        return len(self.items)\n","\n","    def _random_crop(self, img_rs, m_rs, crop_size):\n","        h, w = m_rs.shape\n","        if h < crop_size or w < crop_size:\n","            pad_h = max(0, crop_size - h)\n","            pad_w = max(0, crop_size - w)\n","            img_rs = cv2.copyMakeBorder(img_rs, 0, pad_h, 0, pad_w, borderType=cv2.BORDER_CONSTANT, value=(0,0,0))\n","            m_rs   = np.pad(m_rs, ((0,pad_h),(0,pad_w)), mode=\"constant\", constant_values=0)\n","            h, w = m_rs.shape\n","        y0 = random.randint(0, h - crop_size)\n","        x0 = random.randint(0, w - crop_size)\n","        img_c = img_rs[y0:y0+crop_size, x0:x0+crop_size]\n","        m_c   = m_rs[y0:y0+crop_size, x0:x0+crop_size]\n","        return img_c, m_c\n","\n","    def __getitem__(self, idx):\n","        img_path, json_path = self.items[idx]\n","        img = Image.open(img_path).convert(\"RGB\")\n","        W, H = img.size\n","\n","        gt_mask = make_union_mask(json_path, H, W, line_width=self.line_width, use_aa=self.use_aa)  # (H,W) uint8 0/1\n","\n","        newH, newW = resize_keep_aspect(H, W, self.shortest_edge, self.longest_edge)\n","        img_np = np.array(img)\n","        img_rs = cv2.resize(img_np, (newW, newH), interpolation=cv2.INTER_LINEAR)\n","        m_rs   = cv2.resize(gt_mask, (newW, newH), interpolation=cv2.INTER_NEAREST)\n","\n","        if (not self.return_orig_for_eval) and self.use_fixed_crop:\n","            img_rs, m_rs = self._random_crop(img_rs, m_rs, self.fixed_crop_size)\n","\n","        x = torch.from_numpy(img_rs).permute(2,0,1).float() / 255.0\n","        x = (x - IMAGENET_MEAN) / IMAGENET_STD\n","        y = torch.from_numpy(m_rs).long()\n","\n","        x, pad = pad_to_divisor_tensor(x, divisor=self.size_divisor, pad_value=0.0)\n","        y, _   = pad_to_divisor_mask(y, divisor=self.size_divisor, pad_value=0)\n","\n","        out = {\"pixel_values\": x, \"labels\": y}\n","\n","        if self.return_orig_for_eval:\n","            out[\"orig_hw\"]   = (H, W)\n","            out[\"gt_orig\"]   = torch.from_numpy(gt_mask).to(torch.uint8)\n","            out[\"resized_hw\"]= (newH, newW)\n","            out[\"pad\"]       = pad  # (0,padW,0,padH)\n","        return out\n","\n","def train_collate_fn(batch):\n","    return {\n","        \"pixel_values\": torch.stack([b[\"pixel_values\"] for b in batch], dim=0),\n","        \"labels\": torch.stack([b[\"labels\"] for b in batch], dim=0),\n","    }\n","\n","def val_collate_fn(batch):\n","    b = batch[0]\n","    return {\n","        \"pixel_values\": b[\"pixel_values\"].unsqueeze(0),\n","        \"labels\": b[\"labels\"].unsqueeze(0),\n","        \"orig_hw\": b[\"orig_hw\"],\n","        \"gt_orig\": b[\"gt_orig\"],\n","        \"resized_hw\": b[\"resized_hw\"],\n","        \"pad\": b[\"pad\"],\n","    }\n","\n","# DataLoader prefetch_factor는 num_workers>0에서만 유효\n","dl_common = dict(pin_memory=True, persistent_workers=(NUM_WORKERS > 0))\n","train_loader = DataLoader(\n","    SDLaneSemanticDataset(\n","        train_items, RESIZE_SHORTEST, RESIZE_LONGEST, SIZE_DIVISOR,\n","        line_width=LINE_WIDTH, use_aa=USE_AA, return_orig_for_eval=False,\n","        use_fixed_crop=USE_TRAIN_FIXED_CROP, fixed_crop_size=FIXED_CROP_SIZE\n","    ),\n","    batch_size=TRAIN_BS, shuffle=True, num_workers=NUM_WORKERS,\n","    collate_fn=train_collate_fn, drop_last=True,\n","    prefetch_factor=4 if NUM_WORKERS > 0 else None,\n","    **dl_common\n",")\n","\n","val_loader = DataLoader(\n","    SDLaneSemanticDataset(\n","        val_items, RESIZE_SHORTEST, RESIZE_LONGEST, SIZE_DIVISOR,\n","        line_width=LINE_WIDTH, use_aa=USE_AA, return_orig_for_eval=True,\n","        use_fixed_crop=False\n","    ),\n","    batch_size=1, shuffle=False, num_workers=max(1, NUM_WORKERS//2),\n","    collate_fn=val_collate_fn,\n","    prefetch_factor=2 if max(1, NUM_WORKERS//2) > 0 else None,\n","    pin_memory=True, persistent_workers=True\n",")\n","\n","# ---------------------------\n","# 7) Model\n","# ---------------------------\n","model = smp.DeepLabV3Plus(\n","    encoder_name=ENCODER_NAME,\n","    encoder_weights=ENCODER_WTS,\n","    in_channels=3,\n","    classes=1,\n","    activation=None,\n",").to(device)\n","\n","if USE_CHANNELS_LAST:\n","    model = model.to(memory_format=torch.channels_last)\n","\n","if USE_COMPILE:\n","    try:\n","        model = torch.compile(model, mode=\"reduce-overhead\")\n","        print(\"[Compile] enabled\")\n","    except Exception as e:\n","        print(\"[Compile] failed:\", repr(e))\n","\n","# ---------------------------\n","# 8) Loss: BCEWithLogits + Dice\n","# ---------------------------\n","bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([POS_WEIGHT], device=device))\n","\n","def soft_dice_loss(logits, targets, eps=1e-6):\n","    probs = torch.sigmoid(logits)\n","    tgt = targets.unsqueeze(1).float()\n","    num = 2 * (probs * tgt).sum(dim=(2,3)) + eps\n","    den = (probs + tgt).sum(dim=(2,3)) + eps\n","    return 1.0 - (num / den).mean()\n","\n","def loss_fn(logits, targets):\n","    b = bce(logits, targets.unsqueeze(1).float())\n","    d = soft_dice_loss(logits, targets)\n","    return b + LAMBDA_DICE * d, float(b.detach().cpu()), float(d.detach().cpu())\n","\n","# ---------------------------\n","# 9) Metrics: Dice/IoU + Boundary F1\n","# ---------------------------\n","def dice_iou(pred_bin, gt_bin, eps=1e-6):\n","    pred = pred_bin.astype(bool)\n","    gt   = gt_bin.astype(bool)\n","    inter = np.logical_and(pred, gt).sum()\n","    p = pred.sum(); g = gt.sum()\n","    dice = (2.0 * inter + eps) / (p + g + eps)\n","    iou  = (inter + eps) / (p + g - inter + eps)\n","    return float(dice), float(iou)\n","\n","def boundary_map(bin_mask):\n","    k = np.ones((3,3), np.uint8)\n","    b = cv2.morphologyEx(bin_mask.astype(np.uint8), cv2.MORPH_GRADIENT, k)\n","    return (b > 0).astype(np.uint8)\n","\n","def boundary_f1(pred_bin, gt_bin, tol_px=3):\n","    pred_b = boundary_map(pred_bin)\n","    gt_b   = boundary_map(gt_bin)\n","\n","    pred_pts = pred_b.sum()\n","    gt_pts   = gt_b.sum()\n","    if pred_pts == 0 and gt_pts == 0:\n","        return 1.0, 1.0, 1.0\n","    if pred_pts == 0 or gt_pts == 0:\n","        return 0.0, 0.0, 0.0\n","\n","    dt_gt   = cv2.distanceTransform((1 - gt_b).astype(np.uint8),   cv2.DIST_L2, 3)\n","    dt_pred = cv2.distanceTransform((1 - pred_b).astype(np.uint8), cv2.DIST_L2, 3)\n","\n","    pred_match = (pred_b == 1) & (dt_gt <= tol_px)\n","    gt_match   = (gt_b == 1)   & (dt_pred <= tol_px)\n","\n","    precision = pred_match.sum() / (pred_pts + 1e-6)\n","    recall    = gt_match.sum()   / (gt_pts + 1e-6)\n","    f1 = (2 * precision * recall) / (precision + recall + 1e-6)\n","    return float(precision), float(recall), float(f1)\n","\n","@torch.no_grad()\n","def evaluate(model, val_loader, tols=(2,4,8)):\n","    model.eval()\n","    dices, ious = [], []\n","    bf1 = {t: [] for t in tols}\n","\n","    for batch in tqdm(val_loader, desc=\"Val\", leave=False):\n","        x = batch[\"pixel_values\"].to(device, non_blocking=True)\n","        if USE_CHANNELS_LAST:\n","            x = x.to(memory_format=torch.channels_last)\n","\n","        if device.type == \"cuda\":\n","            with torch.autocast(device_type=\"cuda\", dtype=AMP_DTYPE, enabled=True):\n","                logits = model(x)  # (1,1,h_pad,w_pad)\n","        else:\n","            logits = model(x)\n","\n","        # unpad -> resized -> upsample to orig for metric (패딩 영향 최소화)\n","        (newH, newW) = batch[\"resized_hw\"]\n","        (H, W)       = batch[\"orig_hw\"]\n","        gt_orig = batch[\"gt_orig\"].numpy().astype(np.uint8)\n","\n","        logits_rs = logits[:, :, :newH, :newW]  # remove pad\n","        pred_up = F.interpolate(logits_rs, size=(H, W), mode=\"bilinear\", align_corners=False)\n","        pred_prob = torch.sigmoid(pred_up.float())[0,0].detach().cpu().numpy()\n","        pred_bin = (pred_prob >= 0.5).astype(np.uint8)\n","\n","        d, j = dice_iou(pred_bin, gt_orig)\n","        dices.append(d); ious.append(j)\n","        for t in tols:\n","            _, _, f1 = boundary_f1(pred_bin, gt_orig, tol_px=t)\n","            bf1[t].append(f1)\n","\n","    out = {\"dice\": float(np.mean(dices)), \"iou\": float(np.mean(ious))}\n","    for t in tols:\n","        out[f\"bf1@{t}px\"] = float(np.mean(bf1[t])) if bf1[t] else 0.0\n","    return out\n","\n","# ---------------------------\n","# 10) Saving: ckpt / best.pt / history\n","# ---------------------------\n","history = []\n","history_jsonl = os.path.join(RUN_DIR, \"history.jsonl\")\n","history_json  = os.path.join(RUN_DIR, \"history.json\")\n","\n","def _atomic_write_json(path, obj):\n","    tmp = path + \".tmp\"\n","    with open(tmp, \"w\") as f:\n","        json.dump(obj, f, indent=2)\n","    os.replace(tmp, path)\n","\n","def append_history(row):\n","    history.append(row)\n","    with open(history_jsonl, \"a\") as f:\n","        f.write(json.dumps(row) + \"\\n\")\n","    _atomic_write_json(history_json, history)\n","\n","def save_ckpt(tag, epoch, metrics=None, extra=None, optimizer=None, scheduler=None, best_score=None):\n","    pt = os.path.join(CKPT_DIR, f\"{tag}.pt\")\n","    payload = {\n","        \"epoch\": epoch,\n","        \"model_state\": model.state_dict(),\n","        \"optim_state\": optimizer.state_dict() if optimizer is not None else None,\n","        \"sched_state\": scheduler.state_dict() if scheduler is not None else None,\n","        \"metrics\": metrics or {},\n","        \"cfg\": cfg,\n","        \"extra\": extra or {},\n","        \"best_score\": best_score,\n","        \"saved_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n","    }\n","    torch.save(payload, pt)\n","\n","# ---------------------------\n","# 11) Optim / sched\n","# ---------------------------\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n","\n","best_score = -1.0  # select by bf1@4px\n","global_step = 0\n","\n","# ---------------------------\n","# 12) Profiling helpers\n","# ---------------------------\n","def smi_snapshot():\n","    try:\n","        out = subprocess.check_output(\n","            [\"nvidia-smi\",\"--query-gpu=utilization.gpu,memory.used,memory.total\",\"--format=csv,noheader,nounits\"]\n","        ).decode().strip()\n","        return out\n","    except Exception:\n","        return \"n/a\"\n","\n","def cuda_time_ms(fn):\n","    if device.type != \"cuda\":\n","        t0 = time.time()\n","        fn()\n","        return (time.time() - t0) * 1000.0\n","    starter = torch.cuda.Event(enable_timing=True)\n","    ender   = torch.cuda.Event(enable_timing=True)\n","    starter.record()\n","    fn()\n","    ender.record()\n","    torch.cuda.synchronize()\n","    return starter.elapsed_time(ender)\n","\n","# ---------------------------\n","# 13) Train loop (with profiling)\n","# ---------------------------\n","for epoch in range(1, EPOCHS + 1):\n","    try:\n","        model.train()\n","        epoch_loss = 0.0\n","        epoch_bce  = 0.0\n","        epoch_dice = 0.0\n","        n_steps = 0\n","\n","        it = iter(train_loader)\n","        steps_per_epoch = len(train_loader)\n","        pbar = tqdm(range(steps_per_epoch), desc=f\"Train E{epoch}\", leave=True)\n","\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        for step_idx in pbar:\n","            # (A) fetch\n","            t_fetch0 = time.time()\n","            try:\n","                batch = next(it)\n","            except StopIteration:\n","                it = iter(train_loader)\n","                batch = next(it)\n","            t_fetch = time.time() - t_fetch0\n","\n","            # (B) H2D\n","            if device.type == \"cuda\":\n","                torch.cuda.synchronize()\n","            t_h2d0 = time.time()\n","\n","            x = batch[\"pixel_values\"].to(device, non_blocking=True)\n","            y = batch[\"labels\"].to(device, non_blocking=True)\n","            if USE_CHANNELS_LAST:\n","                x = x.to(memory_format=torch.channels_last)\n","\n","            if device.type == \"cuda\":\n","                torch.cuda.synchronize()\n","            t_h2d = time.time() - t_h2d0\n","\n","            # (C) fwd+bwd\n","            if device.type == \"cuda\":\n","                torch.cuda.synchronize()\n","            t_comp0 = time.time()\n","\n","            if device.type == \"cuda\":\n","                with torch.autocast(device_type=\"cuda\", dtype=AMP_DTYPE, enabled=True):\n","                    logits = model(x)\n","                    loss, bce_v, dice_v = loss_fn(logits, y)\n","                    loss_scaled = loss / GRAD_ACCUM_STEPS\n","            else:\n","                logits = model(x)\n","                loss, bce_v, dice_v = loss_fn(logits, y)\n","                loss_scaled = loss / GRAD_ACCUM_STEPS\n","\n","            if USE_SCALER:\n","                scaler.scale(loss_scaled).backward()\n","            else:\n","                loss_scaled.backward()\n","\n","            if device.type == \"cuda\":\n","                torch.cuda.synchronize()\n","            t_comp = time.time() - t_comp0\n","\n","            # (D) optimizer step (accum boundary)\n","            did_step = False\n","            t_opt_ms = 0.0\n","            if (step_idx + 1) % GRAD_ACCUM_STEPS == 0:\n","                def _opt():\n","                    if USE_SCALER:\n","                        scaler.unscale_(optimizer)\n","                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","                    if USE_SCALER:\n","                        scaler.step(optimizer)\n","                        scaler.update()\n","                    else:\n","                        optimizer.step()\n","\n","                t_opt_ms = cuda_time_ms(_opt)\n","                optimizer.zero_grad(set_to_none=True)\n","                did_step = True\n","\n","            # accumulate stats\n","            epoch_loss += float(loss.detach().cpu())\n","            epoch_bce  += float(bce_v)\n","            epoch_dice += float(dice_v)\n","            n_steps += 1\n","            global_step += 1\n","\n","            iter_t = t_fetch + t_h2d + t_comp + (t_opt_ms/1000.0 if did_step else 0.0)\n","            imgs_s = TRAIN_BS / max(1e-9, iter_t)\n","\n","            if (step_idx + 1) % PROFILE_EVERY == 0:\n","                mem_gb = torch.cuda.memory_allocated()/1024**3 if device.type==\"cuda\" else 0.0\n","                pbar.set_postfix({\n","                    \"loss\": f\"{epoch_loss/n_steps:.4f}\",\n","                    \"fetch_s\": f\"{t_fetch:.3f}\",\n","                    \"h2d_s\": f\"{t_h2d:.3f}\",\n","                    \"comp_s\": f\"{t_comp:.3f}\",\n","                    \"opt_ms\": f\"{t_opt_ms:.1f}\",\n","                    \"iter_s\": f\"{iter_t:.3f}\",\n","                    \"imgs/s\": f\"{imgs_s:.1f}\",\n","                    \"memGB\": f\"{mem_gb:.1f}\",\n","                })\n","\n","            if (step_idx + 1) % SMI_EVERY == 0:\n","                print(f\"[nvidia-smi] step={step_idx+1}/{steps_per_epoch} | {smi_snapshot()}\")\n","\n","        scheduler.step()\n","\n","        avg_loss = epoch_loss / max(1, n_steps)\n","        avg_bce  = epoch_bce  / max(1, n_steps)\n","        avg_dice = epoch_dice / max(1, n_steps)\n","\n","        # save epoch ckpt\n","        save_ckpt(\n","            tag=f\"epoch_{epoch:03d}\",\n","            epoch=epoch,\n","            optimizer=optimizer,\n","            scheduler=scheduler,\n","            best_score=float(best_score),\n","            extra={\n","                \"train_loss\": avg_loss,\n","                \"train_bce\": avg_bce,\n","                \"train_dice_loss\": avg_dice,\n","                \"grad_accum_steps\": GRAD_ACCUM_STEPS,\n","                \"use_train_fixed_crop\": USE_TRAIN_FIXED_CROP,\n","                \"fixed_crop_size\": FIXED_CROP_SIZE if USE_TRAIN_FIXED_CROP else None,\n","                \"amp_dtype\": str(AMP_DTYPE),\n","                \"use_scaler\": USE_SCALER,\n","            }\n","        )\n","\n","        # eval\n","        metrics = evaluate(model, val_loader, tols=BND_TOLS)\n","        sel = float(metrics.get(\"bf1@4px\", metrics[\"dice\"]))\n","\n","        print(\n","            f\"[Epoch {epoch}] train_loss={avg_loss:.4f} (bce={avg_bce:.4f}, diceL={avg_dice:.4f}) | \"\n","            f\"val dice={metrics['dice']:.4f} iou={metrics['iou']:.4f} \"\n","            f\"bf1@2={metrics['bf1@2px']:.4f} bf1@4={metrics['bf1@4px']:.4f} bf1@8={metrics['bf1@8px']:.4f} \"\n","            f\"| sel(bf1@4)={sel:.4f}\"\n","        )\n","\n","        append_history({\n","            \"epoch\": epoch,\n","            \"global_step\": global_step,\n","            \"train_loss\": avg_loss,\n","            \"train_bce\": avg_bce,\n","            \"train_dice_loss\": avg_dice,\n","            \"lr\": float(optimizer.param_groups[0][\"lr\"]),\n","            **metrics,\n","            \"sel_score\": sel,\n","            \"best_score_before\": float(best_score),\n","            \"grad_accum_steps\": GRAD_ACCUM_STEPS,\n","            \"use_train_fixed_crop\": USE_TRAIN_FIXED_CROP,\n","            \"fixed_crop_size\": FIXED_CROP_SIZE if USE_TRAIN_FIXED_CROP else None,\n","            \"encoder_name\": ENCODER_NAME,\n","            \"encoder_weights\": ENCODER_WTS,\n","            \"amp_dtype\": str(AMP_DTYPE),\n","            \"use_scaler\": USE_SCALER,\n","            \"time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n","        })\n","\n","        # best update -> best.pt로 저장\n","        if sel > float(best_score):\n","            best_score = sel\n","            save_ckpt(\n","                tag=\"best\",\n","                epoch=epoch,\n","                metrics=metrics,\n","                optimizer=optimizer,\n","                scheduler=scheduler,\n","                best_score=float(best_score),\n","                extra={\"train_loss\": avg_loss, \"note\": \"best by bf1@4px\"}\n","            )\n","            print(f\"  -> saved BEST (bf1@4px={best_score:.4f}) to {os.path.join(CKPT_DIR,'best.pt')}\")\n","\n","    except Exception as e:\n","        append_history({\n","            \"epoch\": epoch,\n","            \"global_step\": global_step,\n","            \"error\": repr(e),\n","            \"time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n","        })\n","        save_ckpt(\n","            tag=\"crash_last\",\n","            epoch=epoch,\n","            metrics={\"error\": repr(e)},\n","            optimizer=optimizer,\n","            scheduler=scheduler,\n","            best_score=float(best_score),\n","            extra={\"note\": \"crashed during training/eval\"}\n","        )\n","        print(\"[ERROR] crashed; saved crash_last.pt and history. Raising...\")\n","        raise\n","\n","# final\n","save_ckpt(tag=\"final\", epoch=EPOCHS, metrics={\"best_score\": float(best_score)},\n","          optimizer=optimizer, scheduler=scheduler, best_score=float(best_score))\n","print(\"[Done]\")\n","print(\" - RUN_DIR:\", RUN_DIR)\n","print(\" - CKPT_DIR:\", CKPT_DIR)\n","print(\" - history:\", history_jsonl)\n"],"metadata":{"id":"dQaMPgpjRX4w"},"execution_count":null,"outputs":[]}]}